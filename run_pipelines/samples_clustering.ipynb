{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import knpackage.toolbox as kn\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# sys.path.insert(1, '../src')\n",
    "# import KnowEnG_graphics as gu\n",
    "sys.path.insert(1, '../../Data_Cleanup_Pipeline/src')\n",
    "import data_cleanup_toolbox as dc_tbx\n",
    "\n",
    "sys.path.insert(1, '../../Samples_Clustering_Pipeline/src')\n",
    "import sample_clustering_toolbox as sc_tbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def git_clone_Samples_Clustering(pipelines_directory):\n",
    "    \"\"\"  clone samples clustering and data cleaning if they are not installed relative to the calling notebook \"\"\"\n",
    "    working_directory = os.getcwd()\n",
    "    os.chdir(pipelines_directory)\n",
    "\n",
    "    DC_name = 'Data_Cleanup_Pipeline'\n",
    "    Data_Cleanup_Exists = False\n",
    "    SC_name = 'Samples_Clustering_Pipeline'\n",
    "    Samples_Clustering_Exists = False\n",
    "\n",
    "    dir_listing = os.listdir()\n",
    "    for d in dir_listing:\n",
    "        if os.path.isdir(d):\n",
    "            if d == DC_name:\n",
    "                Data_Cleanup_Exists = True\n",
    "            elif d == SC_name:\n",
    "                Samples_Clustering_Exists = True\n",
    "\n",
    "    if Data_Cleanup_Exists == False:\n",
    "        dc_git_string = 'git clone https://github.com/KnowEnG/Data_Cleanup_Pipeline.git'\n",
    "        os.system(dc_git_string)\n",
    "\n",
    "    if Samples_Clustering_Exists == False:\n",
    "        sc_git_string = 'git clone https://github.com/KnowEnG/Samples_Clustering_Pipeline.git'\n",
    "        os.system(sc_git_string)\n",
    "\n",
    "    os.chdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_run_dir(target_dir, REMOVE_RESULTS=False):\n",
    "    \"\"\" setup directory for running a pipeline \"\"\"\n",
    "    if not os.path.isdir(run_dir):\n",
    "        os.mkdir(run_dir)\n",
    "\n",
    "    if os.path.isdir(results_dir) and REMOVE_RESULTS:\n",
    "        os.system('rm ' + results_dir + '/*')\n",
    "    elif not os.path.isdir(results_dir):\n",
    "        os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleanup(button):\n",
    "    global data_cleanup_dict\n",
    "    #     'phenotype_name_full_path':   '../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt',\n",
    "    data_cleanup_dict = {\n",
    "        # 'spreadsheet_name_full_path': '../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df',\n",
    "        # 'gg_network_name_full_path': '../../Samples_Clustering_Pipeline/data/networks/keg_ST90_4col.edge',\n",
    "        'results_directory':          results_dir,\n",
    "        'taxonid':                    '9606',\n",
    "        'source_hint':                '',\n",
    "        'pipeline_type':              'samples_clustering_pipeline',\n",
    "        'redis_credential':\n",
    "                                {'host': 'knowredis.knowhub.org',\n",
    "                                'password': 'KnowEnG',\n",
    "                                'port': '6380'}\n",
    "    }\n",
    "\n",
    "    for w in DC_widget_list:\n",
    "        data_cleanup_dict[w.description[7:-1]] = os.path.join(target_dir,w.value)\n",
    "    # print(data_cleanup_dict)\n",
    "    \n",
    "    SUCCESS, logging = dc_tbx.run_samples_clustering_pipeline(data_cleanup_dict)\n",
    "    if SUCCESS == True:\n",
    "        print('Data Cleanup completed successfully')\n",
    "    else:\n",
    "        print('Data Cleanup Failed - message log:\\n')\n",
    "        for k in logging:\n",
    "            print(k)\n",
    "            \n",
    "    os.listdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59db687905d14e38bfaa9650730664b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5fa75b13464689871515065fb4e1fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe3896bf2a742fea1f68e3437bd6d9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0e6a17a18492a8ead3b69675cba8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleanup Failed - check log messages\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Found 248 intersected gene(s) between phenotype and spreadsheet data.\n",
      "INFO: Finished running sanity check for phenotype data.\n",
      "INFO: Start processing user spreadsheet data.\n",
      "INFO: Start to run sanity checks for user spreadsheet data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Mapped 17490 gene(s) to ensemble name.\n",
      "INFO: Unable to map 57 gene(s) to ensemble name.\n",
      "INFO: Finished running sanity check for user spreadsheet data.\n",
      "INFO: Found 10441 intersected gene(s) between phenotype and spreadsheet data.\n",
      "INFO: Cleaned user spreadsheet has 17490 row(s), 248 column(s).\n",
      "INFO: Cleaned phenotype data has 451 row(s), 19 column(s).\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Found 248 intersected gene(s) between phenotype and spreadsheet data.\n",
      "INFO: Finished running sanity check for phenotype data.\n",
      "INFO: Start processing user spreadsheet data.\n",
      "INFO: Start to run sanity checks for user spreadsheet data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "INFO: Mapped 17490 gene(s) to ensemble name.\n",
      "INFO: Unable to map 57 gene(s) to ensemble name.\n",
      "INFO: Finished running sanity check for user spreadsheet data.\n",
      "INFO: Found 10441 intersected gene(s) between phenotype and spreadsheet data.\n",
      "INFO: Cleaned user spreadsheet has 17490 row(s), 248 column(s).\n",
      "INFO: Cleaned phenotype data has 451 row(s), 19 column(s).\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "ERROR: Cannot find intersection between spreadsheet and phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "ERROR: Cannot find intersection between spreadsheet and phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "ERROR: Cannot find intersection between spreadsheet and phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "ERROR: Cannot find intersection between spreadsheet and phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/UCEC_phenotype.txt with 451 row(s) and 19 column(s)\n",
      "INFO: Start processing phenotype data.\n",
      "INFO: Successfully loaded input data: ../../Samples_Clustering_Pipeline/data/spreadsheets/tcga_ucec_somatic_mutation_data.df with 17547 row(s) and 248 column(s)\n",
      "INFO: Start to run sanity check for phenotype data.\n",
      "INFO: No duplicate column name detected in this data set.\n",
      "INFO: No duplicate row name detected in this data set.\n",
      "ERROR: Cannot find intersection between spreadsheet and phenotype data.\n"
     ]
    }
   ],
   "source": [
    "git_clone_Samples_Clustering(os.path.abspath('../../'))\n",
    "\n",
    "target_dir = '../../Samples_Clustering_Pipeline/data/spreadsheets'\n",
    "\n",
    "run_dir = os.path.join(target_dir, 'run_dir')\n",
    "results_dir = os.path.join(run_dir, 'results')\n",
    "setup_run_dir(target_dir)\n",
    "\n",
    "os.system('cp ../../Samples_Clustering_Pipeline/data/networks/keg_ST90_4col.edge ' + target_dir)\n",
    "\n",
    "#                                         Get list of (docker run -v) mounted files:\n",
    "flist = os.listdir(target_dir)\n",
    "FEXT = ['.tsv', '.txt', '.df','.edge']\n",
    "my_file_list = []\n",
    "for f in flist:\n",
    "    if os.path.isfile(os.path.join(target_dir, f)):\n",
    "        noNeed, f_ext = os.path.splitext(f)\n",
    "        if f_ext in FEXT:\n",
    "            my_file_list.append(f)\n",
    "\n",
    "#                                         (docker run -v) mounted files was empty:\n",
    "if len(my_file_list) <= 0:\n",
    "    my_file_list.append('No Data')\n",
    "    \n",
    "DC_widget_list = []\n",
    "DC_widget_list.append(widgets.Dropdown(\n",
    "    options=my_file_list,\n",
    "    value=my_file_list[0],\n",
    "    description='Select spreadsheet_name_full_path:'\n",
    "))\n",
    "\n",
    "DC_widget_list.append(widgets.Dropdown(\n",
    "    options=my_file_list,\n",
    "    value=my_file_list[0],\n",
    "    description='Select phenotype_name_full_path:'\n",
    "))\n",
    "\n",
    "DC_widget_list.append(widgets.Dropdown(\n",
    "    options=my_file_list,\n",
    "    value=my_file_list[0],\n",
    "    description='Select gg_network_name_full_path:'\n",
    "))\n",
    "\n",
    "for w in DC_widget_list:\n",
    "    display(w)\n",
    "    \n",
    "data_cleanup_button = widgets.Button(\n",
    "    description='Data Cleanup',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='data cleanup button',\n",
    "    )\n",
    "data_cleanup_button.on_click(data_cleanup)\n",
    "display(data_cleanup_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 23\n",
      "b : whodakine\n",
      "c : 44\n"
     ]
    }
   ],
   "source": [
    "try_dict = {'a': 23, 'b':'whodakine', 'c': 44}\n",
    "for k, v in try_dict.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# samples_cluster_dict = {\n",
    "#             'method': 'cc_net_nmf',\n",
    "#             'spreadsheet_name_full_path': '../test/run_dir/results/tcga_ucec_somatic_mutation_data_ETL.tsv',\n",
    "#             'phenotype_name_full_path':   '../test/run_dir/results/UCEC_phenotype_ETL.tsv',\n",
    "#             'threshold': '10',\n",
    "#             'gg_network_name_full_path':  '../data/networks/keg_ST90_4col.edge',\n",
    "#             'results_directory':          '../../user_data/run_dir/results',\n",
    "#             'tmp_directory':               '../../user_data/run_dir',\n",
    "#             'rwr_max_iterations':         '100',\n",
    "#             'rwr_convergence_tolerence':  '1.0e-4',\n",
    "#             'rwr_restart_probability':    '0.7',\n",
    "#             'rows_sampling_fraction':     '0.8',\n",
    "#             'cols_sampling_fraction':     '0.8',\n",
    "#             'number_of_bootstraps':       '4',\n",
    "#             'number_of_clusters':         '3',\n",
    "#             'nmf_conv_check_freq':        '50',\n",
    "#             'nmf_max_invariance':         '200',\n",
    "#             'nmf_max_iterations':         '10000',\n",
    "#             'nmf_penalty_parameter':      '1400',\n",
    "#             'top_number_of_genes':        '100',\n",
    "#             'processing_method':          'parallel',\n",
    "#             'parallelism':                '4'\n",
    "#         }\n",
    "\n",
    "method_list = ['nmf', 'cc_nmf', 'net_nmf', 'cc_net_nmf']\n",
    "threshold_range = {'low':2, 'high':100, 'tip':'categorical vs numerical cutoff threshold'}\n",
    "rwr_max_iterations_range = {'low':2, 'high':1000, 'tip':'random walk no convergence iteration limit'}\n",
    "rwr_convergence_tolerence_range = {'low':1.0e-16, 'high':1000, 'tip':'minimum norm difference'}\n",
    "rwr_restart_probability_range = {'low':0, 'high':1, 'tip': 'Vn+1 = alpha * N * Vn + (1-alpha) * Vo'}\n",
    "rows_sampling_fraction_range = {'low':0, 'high':1, 'tip': 'bootstrap sampling fraction of spreadsheet rows'}\n",
    "cols_sampling_fraction_range = {'low':0, 'high':1, 'tip': 'bootstrap sampling fraction of spreadsheet columns'}\n",
    "number_of_bootstraps_range = {'low':1, 'high':2000, 'tip': 'more bootstrap samples == more run time'}\n",
    "number_of_clusters_range = {'low':2, 'high':12, 'tip': 'more clusters == more run time'}\n",
    "\n",
    "# optional parameters\n",
    "nmf_conv_check_freq_range = {'low':1, 'high':1000, 'tip': 'more frequent checks == more run time'}\n",
    "# nmf_max_invariance_range = \n",
    "\n",
    "# available clusters: AWS, CS Cluster\n",
    "# available methods: serial, parallel, distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def samples_clustering(button):\n",
    "    spreadsheet_name_post_clean = os.path.splitext(DC_widget_list[0].value)[0] + '_ETL.tsv'\n",
    "    phenotype_name_post_clean = os.path.splitext(DC_widget_list[1].value)[0] + '_ETL.tsv'\n",
    "    spreadsheet_name_post_clean = os.path.join(results_dir, spreadsheet_name_post_clean)\n",
    "    phenotype_name_post_clean = os.path.join(results_dir, phenotype_name_post_clean)\n",
    "    try:\n",
    "        samples_cluster_dict = {\n",
    "                 'spreadsheet_name_full_path': spreadsheet_name_post_clean,\n",
    "                 'phenotype_name_full_path':   phenotype_name_post_clean,\n",
    "                 'gg_network_name_full_path':  data_cleanup_dict['gg_network_name_full_path'],\n",
    "                 'results_directory':          results_dir,\n",
    "                 'run_directory':              run_dir,\n",
    "        }\n",
    "\n",
    "        for w in SC_widget_list:\n",
    "            if not isinstance(w,widgets.Label):\n",
    "                samples_cluster_dict[w.description[7:-1]] = w.value\n",
    "\n",
    "        if samples_cluster_dict['method'] == 'cc_net_nmf':\n",
    "            sc_tbx.run_cc_net_nmf(samples_cluster_dict)\n",
    "    except NameError:\n",
    "        print('You should run Data Cleanup Pipeline first! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b33762d38146ec872a2a1fb948aa9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e58d19ceb14fc187c3dcce1842fa05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb3a828c61b4ec38ec36280fbd5022f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ea912652394ec7bec5f454f9c3d1b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3025ac6151545879e754cfe3a1c434b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2990412fac784ffabf8303091c0295cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38a74ce151249eea90a4b6f5c96c177"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564b5b83ef324b3e8984c1b830e2b7ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d124d2e484fe09dd33e7bb4208e6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db48cd8abc045b4897b9ae2ed7042df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d72335b05c74fefbfffbf837f774f4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7732bf6008b14146b9fa6f1d6f3b640b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b99cbfcf5a45b6a2f9655b5fdd1383"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5b052615014ec993c66c862be16229"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error during reading input file ../../Samples_Clustering_Pipeline/data/spreadsheets/run_dir/results/tcga_ucec_somatic_mutation_data_ETL.tsv: <class 'FileNotFoundError'>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../../Samples_Clustering_Pipeline/data/spreadsheets/run_dir/results/tcga_ucec_somatic_mutation_data_ETL.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-eb46e5422661>\u001b[0m in \u001b[0;36msamples_clustering\u001b[0;34m(button)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamples_cluster_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cc_net_nmf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0msc_tbx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cc_net_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_cluster_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You should run Data Cleanup Pipeline first! '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/KnowEng/Samples_Clustering_Pipeline/src/sample_clustering_toolbox.py\u001b[0m in \u001b[0;36mrun_cc_net_nmf\u001b[0;34m(run_parameters)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mlap_diag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlap_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform_network_laplacian_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mspreadsheet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_spreadsheet_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspreadsheet_name_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mspreadsheet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_spreadsheet_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspreadsheet_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_gene_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mspreadsheet_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspreadsheet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/knpackage/toolbox.py\u001b[0m in \u001b[0;36mget_spreadsheet_df\u001b[0;34m(spreadsheet_name_full_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mspreadsheet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspreadsheet_name_full_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         print(\"Unexpected error during reading input file {}: {}\".format(spreadsheet_name_full_path,\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xikun/.pyenv/versions/anaconda3-4.3.1/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../../Samples_Clustering_Pipeline/data/spreadsheets/run_dir/results/tcga_ucec_somatic_mutation_data_ETL.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "SC_widget_list = []\n",
    "SC_widget_list.append(widgets.Dropdown(\n",
    "    options=method_list,\n",
    "    value='cc_net_nmf',\n",
    "    description='Select method:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=2, \n",
    "    max=100,  \n",
    "    value=10, \n",
    "    description='Select threshold:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=2, \n",
    "    max=1000, \n",
    "    value=100, \n",
    "    description='Select rwr_max_iterations:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.BoundedFloatText(\n",
    "    min=1.0e-16, \n",
    "    max=1000,  \n",
    "    value=1e-4, \n",
    "    description='Select rwr_convergence_tolerence:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.FloatSlider(\n",
    "    min=0, \n",
    "    max=1,  \n",
    "    value=0.7, \n",
    "    description='Select rwr_restart_probability:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.FloatSlider(\n",
    "    min=0, \n",
    "    max=1,  \n",
    "    value=0.8, \n",
    "    description='Select rows_sampling_fraction:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.FloatSlider(\n",
    "    min=0, \n",
    "    max=1, \n",
    "    value=0.8, \n",
    "    description='Select cols_sampling_fraction:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=1, \n",
    "    max=2000, \n",
    "    value=4, \n",
    "    description='Select number_of_bootstraps:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=2, \n",
    "    max=12,  \n",
    "    value=3, \n",
    "    description='Select number_of_clusters:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.Label(\n",
    "    value='Optional Parameters: '\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=1, \n",
    "    max=1000,\n",
    "    value=50, \n",
    "    description='Select nmf_conv_check_freq:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.Dropdown(\n",
    "    options=['serial', 'parallel', 'distribute'],\n",
    "    value='parallel',\n",
    "    description='Select processing_method:'\n",
    "))\n",
    "\n",
    "SC_widget_list.append(widgets.IntSlider(\n",
    "    min=1, \n",
    "    max=4,  \n",
    "    value=4, \n",
    "    description='Select parallelism:'\n",
    "))\n",
    "\n",
    "for w in SC_widget_list:\n",
    "    display(w)\n",
    "        \n",
    "samples_clustering_button = widgets.Button(\n",
    "    description='Samples Clustering',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='samples clustering button',\n",
    "    )\n",
    "samples_clustering_button.on_click(samples_clustering)\n",
    "display(samples_clustering_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
